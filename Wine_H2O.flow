{
  "version": "1.0.0",
  "cells": [
    {
      "type": "md",
      "input": "### * **Welcome to H2O Flow!** *\n\n# How to Clasify Wine Quality Like an Expert\n\n#### In this demo we will show you how to:\n\n* **Import a dataset**\n\n* **Parse the dataset's contents**\n\n* **Split your dataset into a training and validation set (more on this later)**\n\n* **Build a model to classify white wines**\n\n* **Evaluate the model's performance**\n\n* **Visualize your results**\n\n* **Extract results from the REST API to make custom visualizations**"
    },
    {
      "type": "md",
      "input": "### How to use the demo\na) you can run all the cells at once by clicking on the double arrowheads next to the question mark in the symbols menu bar at the top of the page\n\nb) or you can click on individual cells and press shift + enter to run a cell (for more information read through the help menu on the left panel)\n\n**Note:** The demo is written so that you do not need to interact with any of the *Action* buttons, however, if you do click a button - to further explore the data - the new cell will appear at the bottom of the page. You can move this new cell up or down using the up or down arrows in the symbols menu bar.\n\n### Summary of the dataset\n* multi-class classification: predict to which class a wine belongs\n* all features are real numbers\n\n*The original dataset can be found at https://archive.ics.uci.edu/ml/datasets/Wine+Quality*\n\n### Step 1) \n#### Import your File\nIn the cell below we import a wine quality dataset from the UCI Machine Learning datset repository. This dataset uses a selection of wine features to predict the quality of a white wine as judged by wine experts. The wines are rated as a 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, or 10 quality wine - 0 being terrible and 10 being excellent.\n\n**Note:** If you do not want to download the dataset from Amazon S3, simply comment out this line, then uncomment the line below, which gives an example of a local path, and replace the local path with the path to your dataset.\n"
    },
    {
      "type": "cs",
      "input": "importFiles"
    },
    {
      "type": "cs",
      "input": "# importFiles [ \"https://s3.amazonaws.com/h2o-public-test-data/smalldata/wine/winequality-white.csv\" ]\nimportFiles [ \"/Users/laurend/Desktop/meetup_10:25/winequality-white.csv\" ]"
    },
    {
      "type": "md",
      "input": "### Step 2)\n#### Parse the File\nH2O Flow parses the csv and reads every column as a numeric value, however, since the 'quality' column is our classification response column, we clicked on the drop-down menu and selected 'Enum' (an enum is a categorical feature, this lets the algorithm know that each number represents a class of wine quality not a real number)."
    },
    {
      "type": "cs",
      "input": "# setupParse source_frames: [ \"https://s3.amazonaws.com/h2o-public-test-data/smalldata/wine/winequality-white.csv\" ]\nsetupParse source_frames: [ \"nfs://Users/laurend/Desktop/meetup_10:25/winequality-white.csv\" ]"
    },
    {
      "type": "cs",
      "input": "parseFiles\n  # source_frames: [\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/wine/winequality-white.csv\"]\n  source_frames: [\"nfs://Users/laurend/Desktop/meetup_10:25/winequality-white.csv\"]\n  destination_frame: \"winequality_white.hex\"\n  parse_type: \"CSV\"\n  separator: 59\n  number_columns: 12\n  single_quotes: false\n  column_names: [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\",\"quality\"]\n  column_types: [\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Numeric\",\"Enum\"]\n  delete_on_done: true\n  check_header: 1\n  chunk_size: 4194304"
    },
    {
      "type": "cs",
      "input": "getFrameSummary \"winequality_white.hex\""
    },
    {
      "type": "md",
      "input": "### Distributions\nYou can look at the distribution of a feature column by clicking on its name in the COLUMN SUMMARIES table above. After clicking on a column's name, a new cell will apear with a characterics bar, boxplot, and histogram. \n\nLet's take a look at the response column. We see that the wine experts are a little snobby when it comes to the wine; most of the wines get the average rating of a 6 out of 10, and none actually get the perfect 10 score. In general it seems like this is a decent wine select with few wines getting poor rating like a 0, 1, or 2.\n\nAfter clicking on the \"sulphate\" column (numeric) we again see a histogram, which gives us the distribution, and a boxplot, which gives us a quick sense of what the min, 25th quartile, median, 75th quartile, and max values are. \n\n*Note: the characterics bar shows what proportion of the feature columns are missing, zeros, others, etc. Hover over it to see which color relates to which characteristic* "
    },
    {
      "type": "cs",
      "input": "getColumnSummary \"winequality_white.hex\", \"quality\""
    },
    {
      "type": "cs",
      "input": "getColumnSummary \"winequality_white.hex\", \"sulphates\""
    },
    {
      "type": "md",
      "input": "### Step 3)\n#### Split the Data\nBefore we build a model, we want to split the dataset into training, validation, and test datasets. This way we can build a model using one dataset, see how the model performs on an other dataset, and make predictions on the remaining dataset. A good split to use is 70% for the training dataset, 15% for the validation dataset, and 15% for the test dataset."
    },
    {
      "type": "cs",
      "input": "assist splitFrame, \"winequality_white.hex\""
    },
    {
      "type": "cs",
      "input": "splitFrame \"winequality_white.hex\", [0.7,0.15], [\"winequality_white.hex_0.70_train\",\"winequality_white.hex_0.15_valid\",\"winequality_white.hex_0.15_test\"], 1234"
    },
    {
      "type": "md",
      "input": "### Step 4)\n#### Build Your Model\n* For this example we will use a GBM, but you could also choose from any of the other supervised learning algorithms that work for classification problems (i.e., Generalized Linear Model, Distributed Random Forest, Deep Learning, or Naive Bayes).\n\n* For the parameters we select the training_frame as the 70% split and the validation_frame as the 15% split, we leave the parameter nfolds alone because we already created our folds with the training and validation frames.\n\n* The response is \"quality\", which we are trying to predict.\n\n* The rest of the parameters can be left with the default values."
    },
    {
      "type": "cs",
      "input": "assist buildModel, null, training_frame: \"winequality_white.hex\""
    },
    {
      "type": "cs",
      "input": "buildModel 'gbm', {\"model_id\":\"GBM_Wine\",\"training_frame\":\"winequality_white.hex_0.70_train\",\"validation_frame\":\"winequality_white.hex_0.15_valid\",\"nfolds\":0,\"response_column\":\"quality\",\"ignored_columns\":[],\"ignore_const_cols\":true,\"ntrees\":50,\"max_depth\":5,\"min_rows\":10,\"nbins\":20,\"seed\":-1,\"learn_rate\":0.1,\"sample_rate\":1,\"col_sample_rate\":1,\"score_each_iteration\":false,\"score_tree_interval\":0,\"balance_classes\":false,\"max_confusion_matrix_size\":20,\"max_hit_ratio_k\":0,\"nbins_top_level\":1024,\"nbins_cats\":1024,\"r2_stopping\":1.7976931348623157e+308,\"stopping_rounds\":0,\"stopping_metric\":\"AUTO\",\"stopping_tolerance\":0.001,\"max_runtime_secs\":0,\"learn_rate_annealing\":1,\"distribution\":\"AUTO\",\"huber_alpha\":0.9,\"checkpoint\":\"\",\"col_sample_rate_per_tree\":1,\"min_split_improvement\":0.00001,\"histogram_type\":\"AUTO\",\"categorical_encoding\":\"AUTO\",\"build_tree_one_node\":false,\"sample_rate_per_class\":[],\"col_sample_rate_change_per_level\":1,\"max_abs_leafnode_pred\":1.7976931348623157e+308,\"pred_noise_bandwidth\":0}"
    },
    {
      "type": "md",
      "input": "### Step 5)\n#### Evaluate Your Model\n\n**Scoring History Plot:**\n\nThis plot shows how the logloss goes down as we increase the number of trees. We see that the validation dataset improves little after 25 trees. Notice how the training logloss keeps improving after 25 trees, this is an example of the model overfitting the data.\n\n*Logloss is a metric that penalises for misclassification. In general, the higher the logloss value the less accurate the model.*  \n\n**The Variable Importances Chart:**\n\nThis chart shows the relative importance of each feature. We see that the model thinks alcohol, volatile acidity, and free sulfur dioxide are the most predictive features. On the other hand the model thinks density and fixed acidity are less relevant to predicting how a wine expert will judge the wine.\n\n**The Confusion Matrix:**\n\nThis matrix provides the classification error for each class. For example on the training dataset, class 3 was correctly classified 10 out of the 15 times, missclassified as a 5 two times, and missclassified as a 6 three times. This gives class 3 an error rate of .3333 or 5/10. The Total, in the very bottom left box, gives the total missclassifications divided by the total times the model selected each class."
    },
    {
      "type": "cs",
      "input": "getModel \"GBM_Wine\""
    },
    {
      "type": "md",
      "input": "### Step 6)\n#### Make a Prediction\nThis is the step where you would make predictions on a dataset without the answers. In our toy dataset we do know the answers, but we will pretend like we don't. \n\n**Note**: If you pass a test dataset with the response column the algorithm will ignore it; it only pays attention to the features columns."
    },
    {
      "type": "cs",
      "input": "predict model: \"GBM_Wine\""
    },
    {
      "type": "cs",
      "input": "predict model: \"GBM_Wine\", frame: \"winequality_white.hex_0.15_test\", predictions_frame: \"Wine_Prediction\""
    },
    {
      "type": "cs",
      "input": "getFrameData \"Wine_Prediction\""
    },
    {
      "type": "md",
      "input": "For fun here is the original dataset so you can directly inspect how the model performed."
    },
    {
      "type": "cs",
      "input": "getFrameData \"winequality_white.hex_0.15_test\"\n# Instead of remembering the command above, 'getFrameData \"winequality_white.hex_0.15_test\"', you can also view the original test dataset by first clicking on 'List All Frames' under Data in the top menu bar, second clicking on the linked ID 'winequality_white.hex_0.15_test', which will show you a summary of the dataset, and third clicking on the 'View Data' *Action* button."
    }
  ]
}